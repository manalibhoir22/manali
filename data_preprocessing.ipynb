{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "data preprocessing",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manalibhoir22/manali/blob/master/data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElVMBXhlOJ5H"
      },
      "source": [
        "### Cleaning the data \n",
        "Remove ‘\\n’\n",
        "\n",
        "Remove emojis if any\n",
        "\n",
        "Remove punctuation marks\n",
        "\n",
        "Remove extra spaces\n",
        "\n",
        "Remove stopwords — Stopwords are those words which occur very frequently but are not required for analysis as they provide no insights. Removing them will reduce computational load. They include words like I, me, myself, that, him, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keyxNgNwOJ5H"
      },
      "source": [
        "import string\n",
        "import nltk \n",
        "from nltk.corpus import stopwords\n",
        "from nltk import PorterStemmer\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIYwfrvOOJ5I"
      },
      "source": [
        "def deEmojify(text):\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regrex_pattern.sub(r'',text)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZcl2Zs8OJ5I"
      },
      "source": [
        "stop_words = ['in','of','at','a','the','and','is','on','an','they','was','it','i','them','to','these','this','for']"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATVnlRl6OJ5I"
      },
      "source": [
        "def decontracted(phrase):\n",
        "    phrase=re.sub(r\"won't\",\"will not\",phrase)\n",
        "    phrase=re.sub(r\"can't\",\"can not\",phrase)\n",
        "    phrase=re.sub(r\"n\\'t\",\" not\",phrase)\n",
        "    phrase=re.sub(r\"\\'re\",\" are\",phrase)\n",
        "    phrase=re.sub(r\"\\'s\",\" is\",phrase)\n",
        "    phrase=re.sub(r\"\\'d\",\" would\",phrase)\n",
        "    phrase=re.sub(r\"\\'ll\",\" will\",phrase)    \n",
        "    phrase=re.sub(r\"\\'t\",\" not\",phrase)\n",
        "    phrase=re.sub(r\"\\'ve\",\" have\",phrase)\n",
        "    phrase=re.sub(r\"\\'m\",\" am\",phrase)\n",
        "    return phrase"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j52536BqOJ5J"
      },
      "source": [
        "#### USING SNOWBALL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UphxjqZVOJ5J",
        "outputId": "2b5f6e50-65e2-4f7d-e30d-09fca9ee45b1"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "\n",
        "##STEMMING USING SNOWBALL\n",
        "snow_stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "#LEMMATIZATION\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk3ALXriOJ5K"
      },
      "source": [
        "def clean_text(text):\n",
        "    \n",
        "    snow_stemmer = SnowballStemmer(language='english')\n",
        "    \n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    \n",
        "    text = decontracted(text)\n",
        "    \n",
        "    text = deEmojify(text) #remove Emojis\n",
        "    \n",
        "    text_cleaned = \"\".join([x for x in text if x not in string.punctuation]) #remove punctuation\n",
        "    \n",
        "    text_cleaned = re.sub(' +', ' ',text_cleaned) #remove extra spaces\n",
        "    \n",
        "    text_cleaned = text_cleaned.lower() #converting into lower case\n",
        "    \n",
        "    tokens = text_cleaned.split(\" \")\n",
        "    \n",
        "    tokens = [token for token in tokens if token not in stop_words] #taking only those words which are not stop words \n",
        "    \n",
        "    text_cleaned = \" \".join([snow_stemmer.stem(token) for token in tokens])\n",
        "    \n",
        "    text_cleaned = \" \".join([lemmatizer.lemmatize(token) for token in tokens])\n",
        "    \n",
        "    return text_cleaned"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HccpGu77OJ5O",
        "outputId": "c22cc8c5-870b-4f85-8b24-e6a7be31ac43"
      },
      "source": [
        "clean_text(\"i won't review this product i'd've gone for another, I'm veryyyy disappointed her's was good, i've and can't i'll be bad it's\")\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'will not review product would have gone another am veryyyy disappointed her good have can not will be bad'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhW1w_e880rI",
        "outputId": "f9ade499-12cd-4a1b-8144-372768fb2179"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "U9xE_aui6oKs",
        "outputId": "d373293d-ca9e-48d9-d89a-50d3f3bec4fc"
      },
      "source": [
        "text = (\"These are awesome and make my phone look so stylish! I have only used one so far and I've had it on for almost a year! CAN YOU BELIEVE THAT! ONE YEAR!! It's Great quality! :)\")\n",
        "clean_text(text)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'are awesome make my phone look so stylish have only used one so far have had almost year can you believe that one year great quality '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGxG9-fJ6pmc"
      },
      "source": [
        "I have only used one so far and have had it on for almost a year! CAN YOU BELIEVE THAT! ONE YEAR!! Great quality! :)'\n",
        "print(tokenized_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84Ujnr0MDQh4",
        "outputId": "50daf987-6792-4f04-d912-7f168b9afee0"
      },
      "source": [
        "phrase = 'I am meeting him tomorrow at the meeting'\n",
        "for word in phrase.split():\n",
        "    print(word+' --> '+snow_stemmer.stem(word))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I --> i\n",
            "am --> am\n",
            "meeting --> meet\n",
            "him --> him\n",
            "tomorrow --> tomorrow\n",
            "at --> at\n",
            "the --> the\n",
            "meeting --> meet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hrq5p56jgLh"
      },
      "source": [
        "words = ['working','work','worked','run','runs','running','easily','fairly','universal','mice']"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZDlljituWht",
        "outputId": "b1cc75bb-f8e2-4b36-e9ae-01a98bcf05b9"
      },
      "source": [
        "snow_stemmer = SnowballStemmer(language='english')\n",
        "for word in words:\n",
        "    print(f'{word:{10}} {snow_stemmer.stem(word):{10}} {lemmatizer.lemmatize(word)} ')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "working    work       working \n",
            "work       work       work \n",
            "worked     work       worked \n",
            "run        run        run \n",
            "runs       run        run \n",
            "running    run        running \n",
            "easily     easili     easily \n",
            "fairly     fair       fairly \n",
            "universal  univers    universal \n",
            "better     better     better \n",
            "mice       mice       mouse \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP6D21WIueHH",
        "outputId": "deaf6d3f-a08c-4e7f-c955-c9c566871d9f"
      },
      "source": [
        "print(f'\\033[1m Word     Stem      Lemma \\033[0m')\n",
        "\n",
        "phrase = 'I am meeting him tomorrow at the meeting'\n",
        "for word in phrase.split():\n",
        "     print(f'{word:{10}} {snow_stemmer.stem(word):{10}}{lemmatizer.lemmatize(word)}')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m Word     Stem      Lemma \u001b[0m\n",
            "I          i         I\n",
            "am         am        am\n",
            "meeting    meet      meeting\n",
            "him        him       him\n",
            "tomorrow   tomorrow  tomorrow\n",
            "at         at        at\n",
            "the        the       the\n",
            "meeting    meet      meeting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65BVv9u8zYfs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}